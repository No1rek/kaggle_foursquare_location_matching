{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874d278d-de05-4e8d-b06f-7092180cb985",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Foursquare Location Matching\n",
    "https://www.kaggle.com/competitions/foursquare-location-matching\n",
    "\n",
    "#### About notebook\n",
    "This is my first kaggle experience. \n",
    "I've been not proud of my results (top 40% on date of writing) but then I found out that this competition had data leakage so now I'm just confused :)\n",
    "I'm very grateful to kaggle community for utility functions implemented in cython and discovery of unidecode.\n",
    "Also I've used this companies dataset https://www.kaggle.com/datasets/peopledatalabssf/free-7-million-company-dataset\n",
    "\n",
    "#### Challenge description\n",
    "We are given dataset representing geographical locations: restaurants, caffes, shops etc. Same location (identified by Point Of Interest (POI)) can be represented by multiple (from 1 to 100+) entries (identified by unique ID).\n",
    "Our target is to match all ID's by same POI in format:\n",
    "<table>\n",
    "    <th><tr><td><b>id</b></td>\n",
    "        <td><b>matches</b></td></tr></th>\n",
    "    <tr><td>E_000001272c6c5d</td><td>E_000001272c6c5d E_da7fa3963561f8</td></tr>\n",
    "    <tr><td>E_000008a8ba4f48</td><td>E_000008a8ba4f48</td></tr>\n",
    "</table>\n",
    "Quality of our prediction is measured by Mean IoU score. \n",
    "Train dataset contains 1100k+ rows. Test dataset contains 600k rows\n",
    "\n",
    "#### Exploratiove data analysis (summary only)\n",
    "1. We are provided with location name, address, latitude, longtitude, city, state, zip, country, url, phone, categories.\n",
    "2. Name, coordinates and country always present, others often omitted:\n",
    "    > ##### non-nan values share\n",
    "    > categories           0.9137<br>\n",
    "    > city                 0.7373<br>\n",
    "    > address              0.6517<br>\n",
    "    > state                0.6307<br>\n",
    "    > zip                  0.4772<br>\n",
    "    > phone                0.3011<br>\n",
    "    > url                  0.2351<br>\n",
    "3. Most of POI has no more than 1 match (not taking itself into account). It means that it can be useful to manually limit number of predicted matches to decrease average false positive rate. <br> \n",
    "<table>\n",
    "    <th><tr><td><b>Number of matches</b></td>\n",
    "            <td><b>Share %</b></td>\n",
    "            <td><b>Cumulative share %</b></td></tr></th>\n",
    "    <tr><td>1</td><td>37.32</td><td>37.32</td></tr>\n",
    "    <tr><td>2</td><td>48.77</td><td>86.10</td></tr>\n",
    "    <tr><td>3</td><td>6.38</td><td>92.47</td></tr>\n",
    "    <tr><td>4</td><td>2.09</td><td>94.56</td></tr>\n",
    "    <tr><td>5</td><td>1.11</td><td>95.66</td></tr>\n",
    "</table>\n",
    "\n",
    "4. Many names and addresses contain numbers, therefore text preprocessing stage must either extract them into separate feature or convert to words (i.e. 1 - > \"one\", 2 -> \"two\").\n",
    "5. Names and addresses contain multilanguage unicode characters so there must be found method to unify them.\n",
    "6. Sometimes match can be found by comparison not only name-to-name but name-to-addres, for example when some caffe is situated in shopping mall and has POI of that mall. In some cases address of this caffe will be equal to mall's name.\n",
    "7. Most of matches located nearby, nearest neighbors matching with 50 neighbors can cover more than 90% of them.\n",
    "8. POI's countries never intersect, so it's good to group dataset by country before performing matching.\n",
    "\n",
    "#### My approach description\n",
    "\n",
    "##### Training\n",
    "1. Unify text using unidecode, leave only alphabetical characters.\n",
    "2. I've used external dataset of companies to extract company names from \"name\" feature. \n",
    "3. Leave only numbers for phones.\n",
    "4. Take sample of 600k rows (sampling stratified by POI) to model distribution of false positives in test set. Leave others for testing.\n",
    "5. Groupbed by countries, find 50 neighbors by location data and 30 neighbors by name similarity. For name similarity I've used ANN with angular distance over 256-dim word-boundary-3-gram bag-of-words with hash trick.\n",
    "6. Make features, including: angular distance, string similarities, category iou, company_names iou, ranking features, numbers iou.\n",
    "7. Fit LightGBM using GP for hyperparameters optimization on that pairs.\n",
    "8. Grid-searching best threshold amoung 3 approaches: a) one score threshold b) score threshold and max amount of neighbors c) separate threshold for each score rank (e.g 0.4 for 1st, 0.5 for 2nd, 0.6 for 3rd). Best found approach is b), threshold 0.5 and no more than 3 matches including itself.\n",
    "\n",
    "##### Prediction\n",
    "1. Perform same preprocessing and neighbors matching as during train, predict scores and leave final predictions.\n",
    "\n",
    "#### My experience and insights after trying different approaches\n",
    "1. Unidecode works well for romance languages but much worse for languages such as arabian, chinese and so on. So my approach almost never can match company source name and it's proper latin transcription. I think this can be solved either by using different transcriprion libraries for different languages or atleast using external vocabulary for companies with their names in source language.\n",
    "2. Multilanguage models have not worked well for me (I've tried small multilanguage models limited by laptop capabilities). Theis usage is good for categories to find similar ones, but for name matching exact character comparison is more important. From other hand, LM features useally not correlated to BOW similarity, so their usage would definitely increase overall score.\n",
    "3. Phonetics dmetaphone also provides good uncorrelated features which usage can increase final metric more than 1% in my case.\n",
    "4. Angular metric for embedding comparison is much more precise than euclidean or cosine, that's why I had to use hash trick to convert sparce matrics to dense arrays to use ANN on them.\n",
    "5. Matching by name similarity together with location increases matching recall from 91% to almost 98%.\n",
    "6. Ranking features give big perfomance boost, by ranking features I mean: if we have neighbors with name similarity 0.1, 0.2, 0.05, the features value will be 1, 0, 2.\n",
    "7. The most important feature for me became BOW similarity, I could get better score by increasing dimensionality from 256 to 1024 but there is tradeoff of execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0a220-c813-41d6-b844-055f52414309",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/foursqare-matching-comp-libs/textdistance-4.2.2-py3-none-any.whl --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bacd6f-b479-4fab-9a29-ecbf3804e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a181058-9bd3-4b13-9e66-affc1ac59eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '/kaggle/input/foursquare-location-matching/test.csv'\n",
    "COMPANIES_PATH = '/kaggle/input/free-7-million-company-dataset/companies_sorted.csv'\n",
    "MODEL_PATH = '/kaggle/input/foursqare-matching-comp-libs/model.jl'\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "\n",
    "import os, psutil\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "def cpu_stats():\n",
    "    \"\"\" Shows memory usage \"\"\"\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30\n",
    "    return 'memory GB:' + str(np.round(memory_use, 2))\n",
    "\n",
    "\n",
    "# seed everything\n",
    "import os \n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "seed = 2022\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a16e1a9-8903-42b6-a151-5df649bbbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import joblib\n",
    "from unidecode import unidecode\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import numba as nb\n",
    "from difflib import SequenceMatcher\n",
    "import warnings\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import combinations\n",
    "\n",
    "import textdistance\n",
    "from pynndescent import NNDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0fb9fff-f355-4374-9273-e0b6711a536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        # print('func:%r args:[%r, %r] took: %2.4f sec' % (f.__name__, '_', kw, te-ts))\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "\n",
    "def fmt_matches(df, pairs, mask=None):\n",
    "    \"\"\" formats pairs for submission \"\"\"\n",
    "    p = pairs.copy()\n",
    "    if mask is not None: p.loc[~mask, 'id_2'] = ''\n",
    "    matches = p[p.id_1 != p.id_2].sort_values(['id_1', 'id_2']).groupby('id_1', as_index=False).id_2.apply(lambda x: ' '.join(x)) \n",
    "    matches.columns = ['id', 'matches']\n",
    "    matches.matches = matches.matches.str.replace(r' {2,}', '').str.strip()\n",
    "    matches = pd.concat([matches, pd.DataFrame({'id':df.id[~df.id.isin(matches.id).values]})], axis=0)\n",
    "    matches.reset_index(inplace=True, drop=True)\n",
    "    matches.matches.fillna('', inplace=True)\n",
    "    matches['matches'] = (matches['id'] + ' ' + matches['matches']).str.strip()\n",
    "    matches = matches.set_index('id').loc[df.id].reset_index()\n",
    "    return matches\n",
    "\n",
    "def AvgIoUScore(matches_pred, matches_true):\n",
    "    \"\"\" target metric \"\"\"\n",
    "    scores = []\n",
    "    for m1,m2 in zip(matches_pred, matches_true):\n",
    "        m1_ = set(m1.split(' '))\n",
    "        m2_ = set(m2.split(' '))\n",
    "        if len((m1_ | m2_)) == 0: continue\n",
    "        i = len(m1_ & m2_)\n",
    "        u = len(m1_ | m2_)\n",
    "        score = i/u\n",
    "        scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "def compute_score(data, pairs):\n",
    "    \"\"\" applies metric to pairs \"\"\"\n",
    "    x = pd.merge(data, data, suffixes=('_1', '_2'), on='point_of_interest')\n",
    "    T = fmt_matches(x, (x.id_1 !=x.id_2)).sort_values('id')\n",
    "    P = fmt_matches(pairs, pairs.match & (pairs.id_1 != pairs.id_2)).sort_values('id')\n",
    "    print(T.shape, P.shape)\n",
    "    T = T[T.id.isin(P.id)]\n",
    "    t = T.matches.values\n",
    "    p = P.matches.values\n",
    "    print(t.shape, p.shape)\n",
    "    scores = AvgIoUScore(t, p)\n",
    "    return T, P, scores.mean()\n",
    "\n",
    "def get_total_matches(df):\n",
    "    counts = df.groupby('point_of_interest').count()\n",
    "    matches = counts * (counts - 1) // 2\n",
    "    return matches.sum()['id']\n",
    "\n",
    "def get_percent_matches(df, pairs):\n",
    "    \"\"\" I used it to select best number of neighbors \"\"\"\n",
    "    matches_tot = get_total_matches(df)\n",
    "    n_matches = len(pairs.loc[pairs.match]) / 2\n",
    "    print(f'{100 * n_matches / matches_tot}% of {matches_tot} matches found')\n",
    "    return n_matches / matches_tot\n",
    "\n",
    "def preprocess_text(s:pd.Series, keep_commas=False, keep_numbers=False):\n",
    "    # normalize unicode\n",
    "    s_new = s.fillna('').apply(lambda x: unidecode(x)).str.lower()\n",
    "\n",
    "    s_new = s_new.str.replace('&', ' and ')\n",
    "    s_new = s_new.str.replace(r'[^a-z 0-9,]' if keep_commas else r'[^a-z 0-9]', '', regex=True)\n",
    "\n",
    "    # separate numbers\n",
    "    if keep_numbers:\n",
    "        s_new = s_new.str.replace(r'(\\b[a-z]+)([0-9]+\\b)', r'\\1 \\2', regex=True).str.replace(r'(\\b[0-9]+)([a-z]+\\b)', r'\\1 \\2', regex=True)\n",
    "    else: s_new = s_new.str.replace('[0-9]', '', regex=True)\n",
    "    \n",
    "    s_new = s_new.str.replace(' +', ' ', regex=True) \n",
    "    if keep_commas: s_new = s_new.str.replace(' ?, ?', ', ', regex=True)\n",
    "    return s_new.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e12785-4f1c-4d9c-8d4b-6880ad03e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np  # noqa\n",
    "\n",
    "cpdef int FastLCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef int cost\n",
    "    cdef int v1,v2,v3,v4\n",
    "    cdef int[:, :] dp = np.zeros((len(S) + 1, len(T) + 1), dtype=np.int32)\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            cost = (int)(S[i] == T[j])\n",
    "            v1 = dp[i, j] + cost\n",
    "            v2 = dp[i + 1, j]\n",
    "            v3 = dp[i, j + 1]\n",
    "            v4 = dp[i + 1, j + 1]\n",
    "            dp[i + 1, j + 1] = max((v1,v2,v3,v4))\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43702b04-5b85-49bc-83b3-c9c0f99ebe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit('float64[:](float32[:,:], float32[:,:])')\n",
    "def angular(a, b):\n",
    "    assert a.shape == b.shape\n",
    "    n, m = a.shape\n",
    "    res = np.empty(n, dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        s = 0.0\n",
    "        for j in range(m): s += a[i,j] * b[i,j]\n",
    "        if s > 1: s = 1\n",
    "        res[i] = np.arccos(s) / np.pi * 2\n",
    "    return res\n",
    "\n",
    "def angular_impute(a,b,empty_mask,impute=2):\n",
    "    d = angular(a,b)\n",
    "    d[empty_mask] = -1\n",
    "    return d\n",
    "\n",
    "@timing\n",
    "def lcs(A, B, impute_value=-1):\n",
    "    lcs = []\n",
    "    for a,b in zip(A,B):\n",
    "        l = min(len(a), len(b))\n",
    "        if l == 0: lcs.append(impute_value); continue\n",
    "        lcs.append(FastLCS(a,b)/len(a))\n",
    "    return np.array(lcs)\n",
    "\n",
    "@timing\n",
    "def lccs(A,B, impute_value=-1):\n",
    "    # true commong contiguous substring\n",
    "    lccs = []\n",
    "    for a,b in zip(A,B):\n",
    "        l = min(len(a), len(b))\n",
    "        if l == 0: lccs.append(impute_value); continue\n",
    "        lccs.append(SequenceMatcher(None, a, b).quick_ratio())\n",
    "    return np.array(lccs)\n",
    "\n",
    "@timing\n",
    "@nb.njit('float64[:](float32[:], float32[:],float32[:],float32[:])')\n",
    "def haversine(phi_1, lambda_1, phi_2, lambda_2):\n",
    "    a = np.sin((phi_2 - phi_1) / 2.0) ** 2 + np.cos(phi_1) * np.cos(phi_2) * np.sin((lambda_2 - lambda_1) / 2.0) ** 2\n",
    "    return  637.1 * 2 * np.arcsin(np.sqrt(a)) # output distance in kilometers\n",
    "\n",
    "@timing\n",
    "def levenshtein(A, B, impute_value=2):\n",
    "    d = []\n",
    "    for a,b in zip(A, B):\n",
    "        l = max(len(a), len(b))\n",
    "        if l == 0: d.append(impute_value); continue\n",
    "        d.append(textdistance.levenshtein(a,b)/l)\n",
    "    return np.array(d)\n",
    "\n",
    "@timing\n",
    "def iou(A,B, impute_value=-1):\n",
    "    d = []\n",
    "    for a,b in zip(A,B):\n",
    "        u = len(a | b)\n",
    "        if u == 0: d.append(impute_value); continue\n",
    "        i = len(a & b)\n",
    "        d.append(i/u)\n",
    "    return np.array(d)\n",
    "\n",
    "@nb.njit('int32[:](int32[:])')\n",
    "def ranker_inner(index):\n",
    "    idx0 = -1\n",
    "    c = 0\n",
    "    R = np.zeros(index.shape[0], dtype=np.int32)\n",
    "    for i,idx in enumerate(index):\n",
    "        if(idx == idx0):\n",
    "            c += 1\n",
    "        else:\n",
    "            idx0 = idx\n",
    "            c = 0\n",
    "        R[i] = c\n",
    "    return R\n",
    "\n",
    "@timing\n",
    "def ranker(index):\n",
    "    \"\"\" converts features to ranks \"\"\"\n",
    "    return ranker_inner(LabelEncoder().fit_transform(index).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa900d-d597-44ff-8364-108f1a1e6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_iter(df, location_data:np.array, chunk_size=25000, n_neighbors=6, train=False, ann=False):\n",
    "    \"\"\"\n",
    "        finds neighbors\n",
    "        if dataset is small it is faster to generate all combinations\n",
    "        in other case we use NN and ANN.\n",
    "    \"\"\"\n",
    "    if (df.shape[0] <= 50):\n",
    "        comb = list(combinations(df.chunk_id.values, 2))\n",
    "        comb = np.stack(comb).astype(np.uint32)\n",
    "        df_1 = df.iloc[comb[:, 0]].reset_index(drop=True)\n",
    "        df_1.columns += '_1'\n",
    "        df_2 = df.iloc[comb[:, 1]].reset_index(drop=True)\n",
    "        df_2.columns += '_2'\n",
    "        pairs = pd.concat([df_1, df_2], axis=1)\n",
    "        if train:\n",
    "            pairs['match'] = pairs['point_of_interest_1'].values == pairs['point_of_interest_2'].values\n",
    "        else: pairs['match'] = np.nan\n",
    "        yield pairs\n",
    "    else:\n",
    "        if ann:\n",
    "            nn = NNDescent(location_data, n_neighbors=30, metric='true_angular',pruning_degree_multiplier=1.5)\n",
    "            nn.prepare()\n",
    "        else:\n",
    "            nn = NearestNeighbors(n_neighbors=min(n_neighbors, len(df)), \n",
    "                                   algorithm='kd_tree', n_jobs=-1)\n",
    "            nn.fit(location_data)\n",
    "\n",
    "        cols_1 = dict(zip(df.columns, [f'{col}_1' for col in df.columns]))\n",
    "        cols_2 = dict(zip(df.columns, [f'{col}_2' for col in df.columns]))\n",
    "\n",
    "        for i in range(0, df.shape[0], chunk_size):\n",
    "            if ann:\n",
    "                neighbors_array, _ = nn.query(location_data[i:i+chunk_size], n_neighbors, epsilon=0.2)\n",
    "            else:\n",
    "                neighbors_array = nn.kneighbors(location_data[i:i+chunk_size], return_distance=False)\n",
    "\n",
    "            df_1 = df.iloc[i:i+chunk_size].copy()\n",
    "            df_1.reset_index(inplace=True, drop=True)\n",
    "            df_1['orig_index'] = np.arange(df_1.shape[0])\n",
    "            df_1 = pd.concat(n_neighbors * [df_1], ignore_index=True)\n",
    "            df_1.reset_index(inplace=True)\n",
    "            df_1.rename(columns=cols_1, inplace=True)\n",
    "            df_1.sort_values(['orig_index', 'index'], inplace=True)\n",
    "            df_1.drop(columns=['orig_index', 'index'], inplace=True)\n",
    "            df_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            idxs = neighbors_array.flatten()\n",
    "            df_2 = df.iloc[idxs]\n",
    "            df_2.rename(columns=cols_2, inplace=True)\n",
    "            df_2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            pairs = pd.concat([df_1, df_2], axis=1)\n",
    "\n",
    "            del df_1, df_2\n",
    "            gc.collect()\n",
    "            pairs = pairs[pairs.id_1.values != pairs.id_2.values]\n",
    "            pairs.reset_index(drop=True, inplace=True)\n",
    "            if train:\n",
    "                pairs['match'] = pairs['point_of_interest_1'].values == pairs['point_of_interest_2'].values\n",
    "            else: pairs['match'] = np.nan\n",
    "            yield pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb5624-5787-4854-ae15-eebc819ddb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@nb.jit(nopython=False)\n",
    "def hash_tokenize(strings, n_features, expr=r\"(?=([a-z][a-z][a-z]))\", normalize=False, dtype=np.float32):\n",
    "    # implements hash trick, faster than sklearn.HashVectorizer\n",
    "    # I do it manually because it's fast and surely deterministic\n",
    "    expr = re.compile(expr)\n",
    "    cols, rows, data = [], [], []\n",
    "    i = 0\n",
    "    for s in strings:\n",
    "        matches = re.findall(expr, s)\n",
    "        for t in matches:\n",
    "            cols.append(hash(t) % n_features); rows.append(i); data.append(1)\n",
    "        if len(matches) == 0:\n",
    "            cols.append(0); rows.append(i); data.append(0)\n",
    "        i+=1\n",
    "    matrix_shape = (len(strings), n_features)\n",
    "    arr = np.zeros(matrix_shape, dtype=dtype)\n",
    "    sp.coo_matrix((np.array(data, dtype=dtype),(rows, cols)), dtype=dtype, shape=matrix_shape).toarray(out=arr)\n",
    "    # normalize\n",
    "    if normalize:\n",
    "        arr = arr / (np.linalg.norm(arr, axis=1)[:, None] + 1e-12)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edb6e4-71b5-44b6-9796-2739f51318ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_inplace(X, e_name):\n",
    "    # distance\n",
    "    X.loc[:, 'distance'] = haversine(X.phi_1.values, X.lambda_1.values, X.phi_2.values, X.lambda_2.values)\n",
    "    \n",
    "    # ngram distance\n",
    "    X.loc[:, 'name_ngram_dist'] = angular_impute(e_name[X.chunk_id_1.values], e_name[X.chunk_id_2.values], ((X.name_1.str.len() == 0) | (X.name_2.str.len() == 0)).values, 2)\n",
    "    # iou \n",
    "    X.loc[:, 'name_numbers_iou'] = iou(X.name_numbers_1.values, X.name_numbers_2.values)\n",
    "    X.loc[:, 'addr_numbers_iou'] = iou(X.addr_numbers_1.values, X.addr_numbers_2.values)\n",
    "    X.loc[:, 'cats_iou'] = iou(X.cats_1.values, X.cats_2.values)\n",
    "    X.loc[:, 'probable_company'] = iou(X.probable_company_1.values, X.probable_company_2.values)\n",
    "    \n",
    "    #lccs\n",
    "    X.loc[:, 'name_lccs'] = lccs(X['name_1'], X['name_2'])\n",
    "    X.loc[:, 'addr_lccs'] = lccs(X['addr_1'], X['addr_2'])\n",
    "    X.loc[:, 'state_lccs'] = lccs(X['state_1'], X['state_2'])\n",
    "    X.loc[:, 'city_lccs'] = lccs(X['city_1'], X['city_2'])\n",
    "    X.loc[:, 'zip_lccs'] = lccs(X['zip_1'], X['zip_2'])\n",
    "    X.loc[:, 'url_lccs'] = lccs(X['url_1'], X['url_2'])\n",
    "    X.loc[:, 'phone_lccs'] = lccs(X['phone_1'], X['phone_2'])\n",
    "    \n",
    "    # lcs\n",
    "    X.loc[:, 'name_lcs'] = lcs(X['name_1'], X['name_2'])\n",
    "    X.loc[:, 'addr_lcs'] = lcs(X['addr_1'], X['addr_2'])\n",
    "    X.loc[:, 'city_lcs'] = lcs(X['city_1'], X['city_2'])\n",
    "    \n",
    "    \n",
    "    # levenshtein\n",
    "    X.loc[:, 'name_levenshtein'] = levenshtein(X['name_1'], X['name_2'])\n",
    "    X.loc[:, 'addr_levenshtein'] = levenshtein(X['addr_1'], X['addr_2'])\n",
    "    X.loc[:, 'city_levenshtein'] = levenshtein(X['city_1'], X['city_2'])\n",
    "    \n",
    "    X.sort_values(['id_1', 'distance'], inplace=True)\n",
    "    X.loc[:, 'distance_rnk'] = ranker(X.id_1.values)\n",
    "    X.sort_values(['id_1', 'name_lccs'], inplace=True, ascending=False)\n",
    "    X.loc[:, 'name_lccs_rnk'] = ranker(X.id_1.values)\n",
    "    X.sort_values(['id_1', 'name_ngram_dist'], inplace=True, ascending=False)\n",
    "    X.loc[:, 'name_ngram_dist_rnk'] = ranker(X.id_1.values)\n",
    "    X.sort_values(['id_1', 'name_levenshtein'], inplace=True, ascending=False)\n",
    "    X.loc[:, 'name_levenshtein_rnk'] = ranker(X.id_1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1edace-5c1e-45b6-92fe-dc528719c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "data = pd.read_csv(TEST_PATH)\n",
    "for c in ['name', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone', 'categories']:\n",
    "    try: data[c] = data[c].round()\n",
    "    except: ...\n",
    "    data[c] = data[c].fillna('').astype(str)\n",
    "\n",
    "data['table_id'] = np.arange(len(data), dtype=np.int32)\n",
    "\n",
    "data['phi'] = np.radians(data.latitude, dtype=np.float32)\n",
    "data['lambda'] = np.radians(data.longitude, dtype=np.float32)\n",
    "\n",
    "data['name_src'] = data.name\n",
    "data['addr_src'] = data.address\n",
    "\n",
    "data['name_numbers'] = data.name.str.findall(r'[0-9]+').apply(set)\n",
    "data['addr_numbers'] = data.address.str.findall(r'[0-9]+').apply(set)\n",
    "\n",
    "# need to be done on train in prod\n",
    "\n",
    "data['name'] = preprocess_text(data.name_src)\n",
    "data['addr'] = preprocess_text(data.addr_src)\n",
    "data['city'] = preprocess_text(data.city)\n",
    "data['state'] = preprocess_text(data.state)\n",
    "\n",
    "data['phone'] = data.phone.str.replace('[^0-9]', '', regex=True)\n",
    "\n",
    "# split categories\n",
    "data['cats'] = preprocess_text(data.categories, keep_commas=True).str.split(', ').apply(set)\n",
    "\n",
    "#probable company\n",
    "companies = pd.read_csv(COMPANIES_PATH)\n",
    "\n",
    "names = data.name_src.str.lower().str.split(' ').explode()\n",
    "names = names[names.isin(companies.name.values)]\n",
    "data['probable_company'] = names.groupby(by=names.index).apply(set)\n",
    "data.loc[data.probable_company.isna(), 'probable_company'] = data.loc[data.probable_company.isna(), 'probable_company'].apply(lambda x: set()).values\n",
    "\n",
    "del companies\n",
    "gc.collect()\n",
    "\n",
    "data['point_of_interest'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fbe00-2b24-46f1-b8b2-b88d3946c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = joblib.load(MODEL_PATH)\n",
    "\n",
    "x_cols = ['distance',\n",
    "       'name_ngram_dist','name_numbers_iou',\n",
    "       'addr_numbers_iou', 'cats_iou', 'probable_company', 'name_lccs',\n",
    "       'addr_lccs', 'state_lccs', 'city_lccs', 'zip_lccs', 'url_lccs',\n",
    "       'phone_lccs', 'name_lcs', 'addr_lcs', 'city_lcs', 'name_levenshtein',\n",
    "       'addr_levenshtein', 'city_levenshtein', 'distance_rnk', 'name_lccs_rnk',\n",
    "       'name_ngram_dist_rnk', 'name_levenshtein_rnk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0655b7-e48a-4db2-870f-b7f7cf4ed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pairs = []\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "with tqdm(total=data.shape[0]) as pbar:\n",
    "    for country, df in data.groupby('country'):\n",
    "        if df.shape[0] == 1: \n",
    "            pbar.update(1) \n",
    "            continue\n",
    "        chunk_size = min(df.shape[0], 10000)\n",
    "        n_neighbors_loc = min(50, df.shape[0])\n",
    "        n_neighbors_name = min(30, df.shape[0])\n",
    "        \n",
    "        ann = df.shape[0] > 1000\n",
    "        \n",
    "        df['chunk_id'] = np.arange(df.shape[0])\n",
    "        \n",
    "        location_data = df[['latitude', 'longitude']].values\n",
    "        name_embeddings = hash_tokenize(df.name, 256, expr=r\"(?=([a-z]{3,3}))\", normalize=True, dtype=np.float32)\n",
    "        \n",
    "        print(country, df.shape[0], cpu_stats())\n",
    "        \n",
    "        for pairs_loc, pairs_name in zip(nn_iter(df, location_data, chunk_size, n_neighbors_loc, True, False), \\\n",
    "                                         nn_iter(df, name_embeddings, chunk_size, n_neighbors_name, True, ann)):\n",
    "            pairs_ = pd.concat([pairs_loc, pairs_name], axis=0)\n",
    "            pairs_.drop_duplicates(['id_1', 'id_2'], inplace=True)\n",
    "            if pairs_.shape[0] == 0:\n",
    "                pbar.update(df.shape[0]) \n",
    "                continue\n",
    "            pairs_['country'] = country\n",
    "            make_features_inplace(pairs_, name_embeddings)\n",
    "            pairs_['score'] = gbm.predict(pairs_[x_cols])\n",
    "            pairs_ = pairs_[['id_1', 'id_2', 'score', 'match']]\n",
    "            pairs_.sort_values(['id_1', 'score'], inplace=True, ascending=False)\n",
    "            pairs_['rnk'] = pairs_.groupby('id_1').cumcount().values\n",
    "            pairs_ = pairs_[(pairs_.score >= 0.5) & (pairs_.rnk <= 2)]\n",
    "            if pairs_.shape[0] == 0: \n",
    "                pbar.update(df.shape[0]) \n",
    "                continue\n",
    "            pairs.append(pairs_)\n",
    "              \n",
    "            \n",
    "            del pairs_\n",
    "            gc.collect()\n",
    "        pbar.update(df.shape[0])\n",
    "        del location_data, name_embeddings\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e00478-0074-4800-a6ef-0047f480686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if len(pairs) > 0:\n",
    "    submission = fmt_matches(data, pd.concat(pairs, axis=0))\n",
    "else:\n",
    "    submission = data[['id']]\n",
    "    submission['match'] = submission.id.values\n",
    "submission.to_csv(OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
